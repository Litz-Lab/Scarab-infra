# How to install slurm
## High level explanation
To install slurm, you need several worker nodes with the slurm daemon and an authenticator daemon (ex: munge) installed on them. You also need a controller node with the slurm control daemon and authenticator daemon running on it. You can install the slurm daemon on the controller node to have it be a worker too. All these nodes need to have the same slurm.conf file installed on them, and the same munge.key for authenticating communications.

These install directions work on Ubuntu 24.04

## The Litz Lab slurm network (as of Dec 2024)
Bohr1 (worker)\
Bohr3 (worker, controller)\
Bohr5 (worker)

Bohr 2,4 don't have slurm installed as of writing, since they use an older major release of Ubuntu

Where to find munge.key:\
On every slurm node at: `/etc/munge/munge.key`\
On the NFS at: `/soe/aesymons/munge.key` 

Where to find slurm.conf\
On every slurm node at: `/etc/slurm/slurm.conf`\
On the NFS at: `/soe/aesymons/slurm.conf`

## Installing slurm
**Note: you will need sudo privileges**

Please complete the Munge section for every node.\
You need to complete the SLURMD section for every worker node. You may also complete it for the worker node.\
You need to complete the SLURMCTLD section the control node.

### Munge
First you need to install the munge authenticator
A munge key will be generated by the first node, which then needs to be put on all nodes in the slurm network so they can communicate properly.
To install munge, run the following commands:\
`sudo apt-get install libmunge-dev libmunge2 munge`\
`sudo systemctl enable munge`\
`sudo systemctl start munge`

If this is your first node:
Save the munge.key file from `/etc/munge/munge.key` so it can be put on the other nodes, and run `munge -n | unmunge | grep STATUS` to test that it works. Then proceed to installing SLURMD

Otherwise, if you have a munge.key file from the first node:\
`sudo cp munge.key /etc/munge/munge.key`\
`sudo chmod 700 /etc/munge/`\
`sudo chown munge:munge /etc/munge/munge.key`\
`sudo systemctl restart munge`\
`sudo systemctl status munge`\
`munge -n | ssh <user>@<first_slurm_node> unmunge | grep STATUS`

You can check the version of munge with `munged -V`

### SLURMD
This sections assumes you have already installed munge, [as described above](#Munge), on this node.
Next, install the slurm worker daemon using the following command\
`sudo apt install slurmd -y`

Now install your slurm.conf configuration file and start the daemon using the following commands\
`sudo cp slurm.conf /etc/slurm/slurm.conf`\
`sudo systemctl enable slurmd`\
`sudo systemctl start slurmd`

Check that slurmd is running properly with the command:\
`sudo systemctl status slurmd`

If there is an issue, then try making sure that all the required directories exist and have the right permissions by following the ['Fix issues related to non-existent directories' directions](#Troubleshooting).

If you are installing slurmd on a node but NOT slurmctld, you do not get the commands to queue and check on jobs. To get these commands, run the following command:\
`sudo apt install slurm-client`

These commands are used by the scarab slurm runner on the node that it is run on, so make sure this package is installed if the slurm runner script is being used on that machine.

### SLURMCTLD
This section assumes you have followed the [Munge section](#Munge) of the installation guide for the node you wish to install the slurm controller daemon on. 
Install the control package using the following command:\
`sudo apt install slurmctld -y`

Then configure with your slurm.conf file as follows.
Note: this is not necessary on nodes where slurmd is already running\
`sudo cp slurm.conf /etc/slurm/slurm.conf`

Then enable and start the control daemon\
`sudo systemctl enable slurmctld`\
`sudo systemctl start slurmctld`

Finally check that it is running properly with the command:\
`sudo systemctl status slurmd`

If there is an issue, then try making sure that all the required directories exist and have the right permissions by following the ['Fix issues related to non-existent directories' directions](#Troubleshooting). Take special note of the last step specific to the control daemon

## Troubleshooting
### Fix issues related to non-existent directories
This is the most prevalent issue in my experience setting up slurm. The following commands will make sure all required directories definitely exist, with the correct permissions and ownership.

`sudo mkdir -p /var/spool/slurmd`\
`sudo mkdir -p /var/lib/slurm-llnl`\
`sudo mkdir -p /var/lib/slurm-llnl/slurmd`\
`sudo mkdir -p /var/lib/slurm-llnl/slurmctld`\
`sudo mkdir -p /var/run/slurm-llnl `\
`sudo mkdir -p /var/log/slurm-llnl`

`sudo chmod -R 755 /var/spool/slurmd`\
`sudo chmod -R 755 /var/lib/slurm-llnl`\
`sudo chmod -R 755 /var/lib/slurm-llnl/slurmd`\
`sudo chmod -R 755 /var/lib/slurm-llnl/slurmctld`\
`sudo chmod -R 755 /var/run/slurm-llnl `\
`sudo chmod -R 755 /var/log/slurm-llnl`

`sudo chown -R slurm:slurm /var/spool/slurmd`\
`sudo chown -R slurm:slurm /var/lib/slurm-llnl/`\
`sudo chown -R slurm:slurm /var/run/slurm-llnl/`\
`sudo chown -R slurm:slurm /var/log/slurm-llnl/`

If the issue is with the slurmctld controller daemon, then try running the following command as well\
`sudo mkdir -p /var/spool/slurmctld`\
`sudo chmod -R 755 /var/spool/slurmctld`\
`sudo chown -R slurm:slurm /var/spool/slurmctld`

### Daemon Debugging
Some useful ways to figure out what is going wrong with the slurm daemons is from their systemctl status\
`sudo systemctl status slurmd`\
`sudo systemctl status slurmctld`

The daemons can also be launched in the terminal to get a live error log with helpful information. The daemons can be launched as follows:\
`sudo slurmd -Dvvv`\
`sudo slurmctld -Dvvv`

Note that you can increase debugging verbosity by adding more 'v's.

Finally you can check the logs to see if a working slurm daemon is encountering any issues with the network. See the (next section)[#Checking-logs] to see how to access the logs.

# Checking logs
You can check the daemon log files to see any issues it may have starting up, during operation, and to see information about running/previously run tasks. The log file locations are defined in the slurm.conf file. For the Litz Lab cluster, they are currently set as follows:\
Slurmd: `/var/log/slurm-llnl/slurmd.log`\
Slurmctld: `/var/log/slurm-llnl/slurmctld.log`

You need root access to cat the contents of these files.